<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="exp," />





  <link rel="alternate" href="/atom.xml" title="神奇小站" type="application/atom+xml" />






<meta property="og:type" content="article">
<meta property="og:title" content="信息内容安全实验2">
<meta property="og:url" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/index.html">
<meta property="og:site_name" content="神奇小站">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409163141122.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409164613616.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409184925832.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409184959263.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409190406516.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192647571.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192713430.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192810184.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192855779.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409193001889.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411180255840.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411180437845.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411180819848.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181124453.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181203044.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181231329.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181303308.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416153013132.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416161709427.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416165856310.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416171617463.png">
<meta property="og:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416172425856.png">
<meta property="article:published_time" content="2020-04-02T11:17:42.000Z">
<meta property="article:modified_time" content="2020-05-04T07:12:09.519Z">
<meta property="article:author" content="ColaLinN">
<meta property="article:tag" content="exp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409163141122.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/"/>





  <title>信息内容安全实验2 | 神奇小站</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">神奇小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">这里总有神奇的东西</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
    
  
  

  <article class="post post-type-normal true" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://colalinn.github.io/2020/04/02/2020-04-02-inforamtion-security-exp-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ColaLinN">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="神奇小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">信息内容安全实验2</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-02T19:17:42+08:00">
                2020-04-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/exp/" itemprop="url" rel="index">
                    <span itemprop="name">exp</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

		  		  
		  
          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6.3k字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  25分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409163141122.png" alt="image-20200409163141122"></p>
<a id="more"></a>

<p>[TOC]</p>
<h1 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h1><p>IDE：pycharm</p>
<p>python版本：anacoda-&gt;python3.7</p>
<h1 id="实验1-1-分词与词向量化"><a href="#实验1-1-分词与词向量化" class="headerlink" title="实验1.1-分词与词向量化"></a>实验1.1-分词与词向量化</h1><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><h3 id="1-分词"><a href="#1-分词" class="headerlink" title="1.分词"></a>1.分词</h3><p>对于西方拼音语言来讲，词之间有明确的分解符，统计和使用语言模型非常直接，而对于中文，词之间没有明确的分界符。因此需要对句子分词后，才能做自然语言处理。</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409164613616.png" alt="image-20200409164613616"></p>
<p>Python中分分词工具很多，包括盘古分词、Yaha分词、Jieba分词等。</p>
<p>这里选择<strong>Jieba</strong>（结巴）分词作为我们实验的工具</p>
<p><strong>安装</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure>

<p>常用方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入自定义词典  </span></span><br><span class="line">jieba.load_userdict(“字典路径\名称.txt<span class="string">")  </span></span><br><span class="line"><span class="string">#动态修改词典              </span></span><br><span class="line"><span class="string">add_word(word, freq=None, tag=None)</span></span><br><span class="line"><span class="string">del_word(word) </span></span><br><span class="line"><span class="string">#可调节单个词语的词频，使其能（或不能）被分出来</span></span><br><span class="line"><span class="string">suggest_freq(segment, tune=True) </span></span><br><span class="line"><span class="string">#关键词提取</span></span><br><span class="line"><span class="string">#sentence 为待提取的文本; topK默认值是20; </span></span><br><span class="line"><span class="string">#withWeight 为是否一并返回关键词权重值，默认值为 False; allowPOS 仅包括指定词性的词</span></span><br><span class="line"><span class="string">jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())</span></span><br><span class="line"><span class="string">#添加停用词</span></span><br><span class="line"><span class="string">jieba.analyse.set_stop_words(“extra_dict/stop_words.txt”)</span></span><br><span class="line"><span class="string">基于textrank的关键词提取</span></span><br><span class="line"><span class="string">tags = jieba.analyse.textrank(text, topK=5, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))</span></span><br></pre></td></tr></table></figure>



<h3 id="2-词向量化"><a href="#2-词向量化" class="headerlink" title="2.词向量化"></a>2.词向量化</h3><p>自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。</p>
<p>NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation。</p>
<p>这种方法把每个词表示为一个很长的向量。</p>
<p>这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。</p>
<p>​    举个例子：</p>
<p>​    “话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 …]</p>
<p>​    “麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 …]</p>
<p>每个词都是茫茫 0 海中的一个 1。</p>
<p>！但是这种简单的方法有两个缺点：<br>     1.维数灾难<br>     2.“词汇鸿沟”现象：任意两个词之间都是孤立的，无法判断像“话筒”和“麦克”是同义词。</p>
<p>所以，我们需要词向量表示</p>
<p>新的词表示方法叫做<strong>Distributed Representation</strong>（分布式表示）。</p>
<p>这种方法表示词即用一个地位实数向量来表示一个词，如：[0.792, −0.177, −0.107, 0.109, −0.542, …]</p>
<p>语言进行词向量化，可使用<strong>Word2Vec</strong>。</p>
<p>word2vec是google的一个开源工具，能够根据输入的词的集合计算出词与词之间的距离。</p>
<p>它将term转换成向量形式，可以把对文本内容的处理简化为向量空间中的向量运算。</p>
<p>计算出向量空间上的相似度，来表示文本语义上的相似度。</p>
<p>word2vec计算的是余弦值，<strong>距离范围为0-1</strong>之间，值越大代表两个词关联度越高。</p>
<p><strong>安装</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#安装带mkl的版本，下载wheel文件</span><br><span class="line">http:&#x2F;&#x2F;www.lfd.uci.edu&#x2F;~gohlke&#x2F;pythonlibs</span><br><span class="line">#定位到存放.whl文件的文件夹，通过匹配安装对应版本的numpy 和scipy</span><br><span class="line">pip install numpy-1.12.1+mkl-cap36</span><br><span class="line">#安装完后，继续安装genism</span><br><span class="line">pip install -U gensim</span><br></pre></td></tr></table></figure>

<p>为了减少安装中的繁琐，直接在anaconda进行集中安装，安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gensim</span><br></pre></td></tr></table></figure>

<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="1-分词-1"><a href="#1-分词-1" class="headerlink" title="1.分词"></a>1.分词</h3><p>代码如下，主要的都写了注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> jieba.analyse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入自定义词典</span></span><br><span class="line">jieba.load_userdict(<span class="string">"dict_all.txt"</span>)</span><br><span class="line"><span class="comment"># 读入语料库并且分词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_file_cut</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 语料库路径</span></span><br><span class="line">    pathBaidu = <span class="string">"BaiduSpiderCountry\\"</span></span><br><span class="line">    <span class="comment"># 分词结果</span></span><br><span class="line">    resName = <span class="string">"Result_Country.txt"</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(resName):</span><br><span class="line">        os.remove(resName)</span><br><span class="line">    result = codecs.open(resName, <span class="string">'w'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    num = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> num &lt;= <span class="number">100</span>:  <span class="comment"># 5A 200 其它100</span></span><br><span class="line">        name = <span class="string">"%04d"</span> % num   <span class="comment">#文件遍历格式：0001-&gt;0100</span></span><br><span class="line">        fileName = pathBaidu + str(name) + <span class="string">".txt"</span>  <span class="comment">#</span></span><br><span class="line">        source = open(fileName, <span class="string">'r'</span>,encoding=<span class="string">"utf-8"</span>)  <span class="comment">#打开文件</span></span><br><span class="line">        line = source.readline()     <span class="comment">#获得line迭代器</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> line != <span class="string">""</span>:</span><br><span class="line">            line = line.rstrip(<span class="string">'\n'</span>)  <span class="comment"># 删除string字符串末尾的指定字符</span></span><br><span class="line">            seglist = jieba.cut(line, cut_all=<span class="literal">False</span>)  <span class="comment"># 精确模式</span></span><br><span class="line">            output = <span class="string">' '</span>.join(list(seglist))  <span class="comment"># 空格拼接，将元组转换为列表，元组是括号，列表是方括号</span></span><br><span class="line">            result.write(output + <span class="string">' '</span>)  <span class="comment"># 空格取代换行'\r\n'</span></span><br><span class="line">            line = source.readline()   <span class="comment">#下一line</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'End file: '</span> + str(num))  <span class="comment">#line为空，这个文件遍历结束</span></span><br><span class="line">            result.write(<span class="string">'\r\n'</span>)  <span class="comment">#换行'\r\n'</span></span><br><span class="line">            source.close()        <span class="comment">#关闭源</span></span><br><span class="line">        num = num + <span class="number">1</span>  <span class="comment">#下一个文件</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'End BaiduSpiderCountry cut：'</span>+str(num)) <span class="comment"># 结束百度语料库分词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run function</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    read_file_cut()</span><br></pre></td></tr></table></figure>



<p><a href="https://blog.csdn.net/chenj_freedom/article/details/81184296?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" target="_blank" rel="noopener">换行格式：</a></p>
<p>1、文档是windows格式，当我们按下键盘上的“回车键”时，输出的是CR和LF，即0d，0a两个字符。<br>2、文档是unix格式，当我们按下键盘上的“回车键”时，输出的LF，即0a一个字符。<br>3、文档是mac模式，当我们按下键盘上的“回车键”时，输出的是CR，即0d一个字符。</p>
<p>运行~</p>
<p>成功</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409184925832.png" alt="image-20200409184925832"></p>
<p>分词结果如下</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409184959263.png" alt="image-20200409184959263"></p>
<h3 id="1-词向量化"><a href="#1-词向量化" class="headerlink" title="1.词向量化"></a>1.词向量化</h3><p>代码如下，主要的都写了注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span>  logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化配置</span></span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line"><span class="comment"># 加载语料</span></span><br><span class="line">sentences = word2vec.Text8Corpus(<span class="string">"Result_Country.txt"</span>)</span><br><span class="line"><span class="comment"># 训练模型,维度设置为200;</span></span><br><span class="line">model = word2vec.Word2Vec(sentences, size=<span class="number">200</span>) </span><br><span class="line"></span><br><span class="line">print(<span class="string">"阿富汗的词向量："</span>)</span><br><span class="line">print(model[<span class="string">'阿富汗'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"争端、冲突这两个词的相关程度："</span>)</span><br><span class="line">y1 = model.similarity(<span class="string">"争端"</span>,<span class="string">"冲突"</span>)</span><br><span class="line">print(y1)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"输出与“地区”相关度最高的20 个词："</span>)</span><br><span class="line">y2 = model.most_similar(<span class="string">"地区"</span>, topn=<span class="number">20</span>)</span><br><span class="line">print(y2)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"法官 总统 部长 北纬 这四个词中最“不合群”的词"</span>)</span><br><span class="line">y4 = model.doesnt_match(<span class="string">"法官 总统 部长 北纬"</span>.split())</span><br><span class="line">print(y4)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">model.save(<span class="string">"国家.model"</span>)</span><br><span class="line"><span class="comment"># 读取模型</span></span><br><span class="line"><span class="comment"># model_2 = word2vec.Word2Vec.load("国家.model")</span></span><br></pre></td></tr></table></figure>

<p>运行</p>
<p>模型跑成功</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409190406516.png" alt="image-20200409190406516"></p>
<ul>
<li>阿富汗的词向量，一个200维的数：</li>
</ul>
<p>​    <img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192647571.png" alt="image-20200409192647571"></p>
<ul>
<li>争端、冲突这两个词的相关程度：</li>
</ul>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192713430.png" alt="image-20200409192713430"></p>
<ul>
<li>输出与“地区”相关度最高的20 个词：</li>
</ul>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192810184.png" alt="image-20200409192810184"></p>
<ul>
<li>法官 总统 部长 北纬 这四个词中最“不合群”的词</li>
</ul>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409192855779.png" alt="image-20200409192855779"></p>
<p>保存模型</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200409193001889.png" alt="image-20200409193001889"></p>
<h1 id="实验1-2-自选词典数据语料库"><a href="#实验1-2-自选词典数据语料库" class="headerlink" title="实验1.2-自选词典数据语料库"></a>实验1.2-自选词典数据语料库</h1><h2 id="1-选择词典数据语料库"><a href="#1-选择词典数据语料库" class="headerlink" title="1.选择词典数据语料库"></a>1.选择词典数据语料库</h2><ul>
<li>在词典方面，我用<a href="https://pinyin.sogou.com/dict/" target="_blank" rel="noopener">搜狗的细胞词库</a><ul>
<li>下载网络安全词典，使用<a href="http://tools.bugscaner.com/sceltotxt/" target="_blank" rel="noopener">搜狗细胞词库转txt工具</a></li>
<li><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411180255840.png" alt="image-20200411180255840"></li>
</ul>
</li>
<li>在语料库方面，我选择<a href="http://bcc.blcu.edu.cn/" target="_blank" rel="noopener">BBC语料库</a><ul>
<li>如图，在科技板块，搜索相关关键词</li>
<li><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411180437845.png" alt="image-20200411180437845"></li>
<li>大概搜索了五个关键词（如病毒、蠕虫、网络安全等）。</li>
<li>然后根据其语料集存在的问题，进行数据清洗</li>
</ul>
</li>
</ul>
<p>选择词典数据语料库的结果如下</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411180819848.png" alt="image-20200411180819848"></p>
<h2 id="2-分词"><a href="#2-分词" class="headerlink" title="2.分词"></a>2.分词</h2><p>代码与第一个实验基本一致</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">security_cut</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 导入自定义词典</span></span><br><span class="line">    jieba.load_userdict(<span class="string">"security_dict.txt"</span>)</span><br><span class="line">    <span class="comment"># 语料库路径</span></span><br><span class="line">    pathBaidu = <span class="string">"security\\"</span></span><br><span class="line">    <span class="comment"># 分词结果</span></span><br><span class="line">    resName = <span class="string">"Result_Security.txt"</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(resName):</span><br><span class="line">        os.remove(resName)</span><br><span class="line">    result = codecs.open(resName, <span class="string">'w'</span>, <span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line">    num = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> num &lt;= <span class="number">5</span>:  <span class="comment"># 5A 200 其它100</span></span><br><span class="line">        name = <span class="string">"%d"</span> % num   <span class="comment">#文件遍历格式：0001-&gt;0100</span></span><br><span class="line">        fileName = pathBaidu + str(name) + <span class="string">".txt"</span></span><br><span class="line">        source = open(fileName, <span class="string">'r'</span>,encoding=<span class="string">"utf-8"</span>)  <span class="comment">#打开文件</span></span><br><span class="line">        line = source.readline()     <span class="comment">#获得line迭代器</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> line != <span class="string">""</span>:</span><br><span class="line">            line = line.rstrip(<span class="string">'\n'</span>)  <span class="comment"># 删除string字符串末尾的指定字符</span></span><br><span class="line">            seglist = jieba.cut(line, cut_all=<span class="literal">False</span>)  <span class="comment"># 精确模式</span></span><br><span class="line">            output = <span class="string">' '</span>.join(list(seglist))  <span class="comment"># 空格拼接，将元组转换为列表，元组是括号，列表是方括号</span></span><br><span class="line">            result.write(output + <span class="string">' '</span>)  <span class="comment"># 空格取代换行'\r\n'</span></span><br><span class="line">            line = source.readline()   <span class="comment">#下一line</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'End Security file: '</span> + str(num))  <span class="comment">#line为空，这个文件遍历结束</span></span><br><span class="line">            result.write(<span class="string">'\r\n'</span>)  <span class="comment">#换行'\r\n'</span></span><br><span class="line">            source.close()        <span class="comment">#关闭源</span></span><br><span class="line">        num = num + <span class="number">1</span>  <span class="comment">#下一个文件</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'End Security cut'</span>) <span class="comment"># 结束百度语料库分词</span></span><br><span class="line"><span class="comment"># Run function</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    security_cut()</span><br></pre></td></tr></table></figure>

<h2 id="3-词向量化"><a href="#3-词向量化" class="headerlink" title="3.词向量化"></a>3.词向量化</h2><p>代码与第一个实验基本一致</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python file uses the following encoding: utf-8</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span>  logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化配置</span></span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line"><span class="comment"># 加载语料</span></span><br><span class="line">sentences = word2vec.Text8Corpus(<span class="string">"Result_Security.txt"</span>)</span><br><span class="line"><span class="comment"># 训练模型,维度设置为200;</span></span><br><span class="line">model = word2vec.Word2Vec(sentences, size=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"病毒的词向量，一个200维的数："</span>)</span><br><span class="line">print(model[<span class="string">'病毒'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"病毒、木马这两个词的相关程度："</span>)</span><br><span class="line">y1 = model.similarity(<span class="string">"病毒"</span>,<span class="string">"木马"</span>)</span><br><span class="line">print(y1)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"输出与“网络安全”相关度最高的20 个词："</span>)</span><br><span class="line">y2 = model.most_similar(<span class="string">"网络安全"</span>, topn=<span class="number">20</span>)</span><br><span class="line">print(y2)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"病毒 木马 诺顿 蠕虫 这四个词中最“不合群”的词"</span>)</span><br><span class="line">y4 = model.doesnt_match(<span class="string">"病毒 木马 诺顿 蠕虫"</span>.split())</span><br><span class="line">print(y4)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">model.save(<span class="string">"网络安全.model"</span>)</span><br><span class="line"><span class="comment"># 读取模型</span></span><br><span class="line"><span class="comment"># model_2 = word2vec.Word2Vec.load("国家.model")</span></span><br></pre></td></tr></table></figure>

<p>词向量模型训练成功，测试的结果如下</p>
<ul>
<li><p>病毒的词向量，一个200维的数：</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181124453.png" alt="image-20200411181124453"></p>
</li>
<li><p>病毒、木马这两个词的相关程度：0.9492484</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181203044.png" alt="image-20200411181203044"></p>
</li>
<li><p>输出与“网络安全”相关度最高的20 个词：</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181231329.png" alt="image-20200411181231329"></p>
</li>
<li><p>病毒 木马 诺顿 蠕虫 这四个词中最“不合群”的词：诺顿</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200411181303308.png" alt="image-20200411181303308"></p>
</li>
</ul>
<p>#coding = gbk</p>
<h1 id="实验2-垃圾邮件的分类"><a href="#实验2-垃圾邮件的分类" class="headerlink" title="实验2-垃圾邮件的分类"></a>实验2-垃圾邮件的分类</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="1-文本分类"><a href="#1-文本分类" class="headerlink" title="1.文本分类"></a>1.文本分类</h3><p>文本分类就是在给定的分类体系下,让计算机根据给定文本的内容，将其判</p>
<p>别为事先确定的若干个文本类别中的某一类或某几类的过程。</p>
<p>一般来说，文本分类可以分为一下过程：</p>
<p>（1） 预处理：将原始语料格式化为同一格式，便于后续的统一处理；</p>
<p>（2） 索引：将文档分解为基本处理单元，同时降低后续处理的开销；</p>
<p>（3） 统计：词频统计，项（单词、概念）与分类的相关概率；</p>
<p>（4） 特征抽取：从文档中抽取出反映文档主题的特征；</p>
<p>（5） 分类器：分类器的训练；</p>
<p>（6） 评价：分类器的测试结果分析。</p>
<p>典型的分类算法包括Rocchio算法、朴素贝叶斯分类算法、K-近邻算法、决</p>
<p>策树算法、神经网络算法和支持向量机算法等。 </p>
<h3 id="2-朴素贝叶斯分类算法"><a href="#2-朴素贝叶斯分类算法" class="headerlink" title="2.朴素贝叶斯分类算法"></a>2.朴素贝叶斯分类算法</h3><p>根据贝叶斯定理，利用先验概率和条件概率估算后验概率：</p>
<p>先验概率：事情还没有发生,那么这件事情发生的可能性的大小。</p>
<p>后验概率：事情已经发生,那么这件事情发生的原因是由某个因素引起的可能性的大小。</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416153013132.png" alt="image-20200416153013132"></p>
<p>将该算法代入我们的垃圾邮件分类任务中之后，理论如下：</p>
<h4 id="1-概率计算"><a href="#1-概率计算" class="headerlink" title="(1).概率计算"></a>(1).概率计算</h4><p>假设c0是正常邮件，c1是垃圾邮件。<code>𝑝(𝑐0)</code>表示在邮件数据集中，正常邮件的概率，<code>𝑝(𝑐1)</code>则表示垃圾邮件的概率，所以计数之后除以邮件总数即可。</p>
<p>x与y分别是邮件的两个特征，那么，当邮件有x和y两个特征时（因为两个概率的分母完全一样，因此在比较两者大小时可忽略分母。）</p>
<ul>
<li><p>为正常邮件的概率为<code>𝑝(𝑐0/𝑥, 𝑦) =𝑝(𝑥, 𝑦/c0)*p(c0)</code> </p>
</li>
<li><p>为垃圾邮件的概率为<code>𝑝(𝑐1/𝑥, 𝑦) =𝑝(𝑥, 𝑦/c1)*p(c1)</code> </p>
</li>
</ul>
<h4 id="2-“朴素”——引入条件独立性假设"><a href="#2-“朴素”——引入条件独立性假设" class="headerlink" title="(2).“朴素”——引入条件独立性假设"></a>(2).“朴素”——引入条件独立性假设</h4><p>x和y的条件概率相互独立。𝑝(𝑥/𝑐𝑖)和𝑝(𝑦/𝑐𝑖)可以对数据进行计数而直接得出。</p>
<p>则条件概率<code>𝑝(𝑥, 𝑦/ci)=𝑝(𝑥/c0)*𝑝(𝑦/ci)</code></p>
<p>即上述公式为<code>𝑝(𝑐0/𝑥, 𝑦) =𝑝(𝑥, 𝑦/c0)*𝑝(c0)=𝑝(𝑥/c0)*𝑝(𝑦/ci)*p(c0)</code> </p>
<h4 id="3-假设每个样本至少出现一次"><a href="#3-假设每个样本至少出现一次" class="headerlink" title="(3).假设每个样本至少出现一次"></a>(3).假设每个样本至少出现一次</h4><p>由于条件独立性假设，需要对条件概率进行乘法运算，若某个样本不出现，即概率为0，则最后结果也为0。所以假设每个样本至少出现一次。</p>
<h4 id="4-将全部乘法运算改为log运算"><a href="#4-将全部乘法运算改为log运算" class="headerlink" title="(4).将全部乘法运算改为log运算"></a>(4).将全部乘法运算改为log运算</h4><p>在实际运算中，条件概率可能会很小，即接近于0，那么在乘法运算中很可能会有下溢出的问题。</p>
<h3 id="3-实际的编码流程"><a href="#3-实际的编码流程" class="headerlink" title="3.实际的编码流程"></a>3.实际的编码流程</h3><p>在本实验中，实际的编码时，我们所需要做的事按顺序排列的如下：</p>
<ol>
<li><p>网上选择一些中文常用的停用词。</p>
</li>
<li><p>读入spam（恶意）、ham（正常）邮件，用jieba.cut()分词，并且去除停用词，保存。</p>
</li>
<li><p>用jieba.analyse.extract_tags()分别提取两个文件的前50（或更多）常见词，合成为一个常见词list，作为我们的特征词向量features。</p>
</li>
<li><p>接下来，对spam、ham的分词结果进行特征词向量features的特征计算：</p>
<ol>
<li>对每一封邮件，其特征词向量features全为1</li>
<li>统计其在features中每个词的出现次数</li>
<li>如果features出现一次，该项就加1</li>
</ol>
</li>
<li><p>根据朴素贝叶斯定理计算spam、ham后验概率spam_vec\ham_vec</p>
</li>
<li><p>计算spam、ham各占总邮件数的概率p_spam、p_ham</p>
</li>
<li><p>计算待测试集的特征集向量test_vec</p>
</li>
<li><p>将<code>test_vec*spam_vec*p_spam</code> 与<code>test_vec*ham_vec*p_ham</code>相比，</p>
</li>
<li><p>哪方概率大则该封测试邮件属于哪一类（为了避免正常邮件分为垃圾邮件，当概率相等时，判定为正常）</p>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><ol>
<li>网上选择一些中文常用的停用词。</li>
</ol>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416161709427.png" alt="image-20200416161709427"></p>
<ol start="2">
<li><p>读入spam（恶意）、ham（正常）邮件，用jieba.cut()分词，并且去除停用词，保存。</p>
</li>
<li><p>用jieba.analyse.extract_tags()分别提取两个文件的前50（或更多）常见词，合成为一个常见词list，作为我们的特征词向量features。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_tags_f</span><span class="params">(origin_file_name, target_file_name,number_of_item)</span>:</span></span><br><span class="line">    stop_word_file = <span class="string">"stop_word_list.txt"</span>  <span class="comment"># 停用词txt</span></span><br><span class="line">    stop_word = list()  <span class="comment"># 停用词数组</span></span><br><span class="line">    target_file = open(target_file_name, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>)  <span class="comment"># 提取保存的文件</span></span><br><span class="line">    <span class="keyword">with</span> open(stop_word_file, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> stop_word_file_object:</span><br><span class="line">        contents = stop_word_file_object.readlines()</span><br><span class="line">        <span class="comment"># print(contents)</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> contents:</span><br><span class="line">            line = line.strip()  <span class="comment"># 移除尾部字符</span></span><br><span class="line">            stop_word.append(line)</span><br><span class="line">    <span class="comment"># print(stop_word)</span></span><br><span class="line">    origin_file = origin_file_name <span class="comment">#对文件进行逐行遍历分词</span></span><br><span class="line">    s = <span class="string">""</span> <span class="comment">#没有停顿词的文件中每一行的词串</span></span><br><span class="line">    <span class="keyword">with</span> open(origin_file, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> origin_file_obejct:</span><br><span class="line">        contents = origin_file_obejct.readlines() <span class="comment">#读取</span></span><br><span class="line">        <span class="comment"># print(contents)</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> contents:</span><br><span class="line">            line = line.strip() <span class="comment">#将line去除尾部回车换行</span></span><br><span class="line">            out_line = <span class="string">""</span> <span class="comment">#处理后的line</span></span><br><span class="line">            word_list = jieba.cut(line, cut_all=<span class="literal">True</span>) <span class="comment">#分词成list</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> word_list:  <span class="comment">#将list以空格间隔合并</span></span><br><span class="line">                <span class="keyword">if</span> (word <span class="keyword">not</span> <span class="keyword">in</span> stop_word) <span class="keyword">and</span> (word != <span class="string">"\t"</span>): <span class="comment">#去除停用词</span></span><br><span class="line">                    out_line = out_line + word + <span class="string">" "</span></span><br><span class="line">            s = s + out_line <span class="comment">#没有停顿词的文件中每一行的词串</span></span><br><span class="line">            target_file.write(out_line.strip() + <span class="string">"\n"</span>) <span class="comment">#保存分词文件</span></span><br><span class="line">    <span class="comment"># print(s)</span></span><br><span class="line">    features = jieba.analyse.extract_tags(s, number_of_item) <span class="comment">#将分词特征提取</span></span><br><span class="line">    <span class="comment"># print(features)</span></span><br><span class="line">    <span class="comment"># print(len(features))</span></span><br><span class="line">    <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>接下来，对spam、ham的分词结果进行特征词向量features的特征计算：<ol>
<li>对每一封邮件，其特征词向量features全为1</li>
<li>统计其在features中每个词的出现次数</li>
<li>如果features出现一次，该项就加1</li>
</ol>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_vec</span><span class="params">(ham,spam,features)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(ham, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        ham_lines = f1.readlines()</span><br><span class="line">    <span class="keyword">with</span> open(spam, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        spam_lines = f1.readlines()</span><br><span class="line">    list_sum=np.zeros((<span class="number">200</span>,len(features))) <span class="comment">#所有特征词向量，前100为正常邮件，后100为垃圾邮件</span></span><br><span class="line">    list_sum_i=<span class="number">0</span>    <span class="comment">#所有特征词向量的赋值下标i</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ham_lines: <span class="comment">#计算每封正常邮件特征词向量</span></span><br><span class="line">        list_ham = np.ones(len(features)) <span class="comment">#对每一封邮件，其特征词向量features全为1</span></span><br><span class="line">        line = i.split(<span class="string">' '</span>) <span class="comment">#按空格split成一个邮件词list</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(features)): <span class="comment">#对于features中每个词</span></span><br><span class="line">            <span class="keyword">for</span> line_feature <span class="keyword">in</span> line : <span class="comment">#统计邮件中每个词的出现次数</span></span><br><span class="line">                <span class="keyword">if</span> features[j] == line_feature: <span class="comment">#如果features出现一次，该项就加1</span></span><br><span class="line">                    <span class="comment"># print("get")</span></span><br><span class="line">                    list_ham[j]=list_ham[j]+<span class="number">1</span></span><br><span class="line">        list_sum[list_sum_i]=list_ham <span class="comment">#将该封邮件词向量复制给所有特征词向量</span></span><br><span class="line">        list_sum_i +=<span class="number">1</span></span><br><span class="line">    <span class="comment"># print("——————————————————————————————————————————————————————")</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> spam_lines: <span class="comment">#计算垃圾邮件特征词向量</span></span><br><span class="line">        list_spam = np.ones(len(features))</span><br><span class="line">        line = i.split(<span class="string">' '</span>)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(features)):</span><br><span class="line">            <span class="keyword">for</span> line_feature <span class="keyword">in</span> line :</span><br><span class="line">                <span class="keyword">if</span> features[j] == line_feature:</span><br><span class="line">                    <span class="comment"># print("get")</span></span><br><span class="line">                    list_spam[j]=list_spam[j]+<span class="number">1</span></span><br><span class="line">        list_sum[list_sum_i]=list_spam</span><br><span class="line">        list_sum_i +=<span class="number">1</span></span><br><span class="line">    cate1=[<span class="number">0</span>]*<span class="number">100</span>  <span class="comment">#前面100封邮件正常，后面100封邮件垃圾</span></span><br><span class="line">    cate2=[<span class="number">1</span>]*<span class="number">100</span></span><br><span class="line">    cate=cate1+cate2 <span class="comment">#cate为邮件的类别lsit</span></span><br><span class="line">    <span class="keyword">return</span> list_sum,cate</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>根据朴素贝叶斯定理计算spam、ham后验概率spam_vec\ham_vec</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traing_bayes</span><span class="params">(trainMatrix, trainCategory)</span>:</span>  <span class="comment"># trainMatrix为所有邮件的矩阵表示，trainCategory为表示邮件类别的向量</span></span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    numTrainDocs = len(trainMatrix)  <span class="comment"># 邮件总数量</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])  <span class="comment"># 词典长度</span></span><br><span class="line">    pSpam = sum(trainCategory) / float(numTrainDocs)  <span class="comment"># 统计垃圾邮件的总个数，然后除以总文档个数（先验概率）</span></span><br><span class="line">    p0Num = np.ones(numWords)  <span class="comment"># 将向量初始化为1，表示每个词至少出现1次</span></span><br><span class="line">    p1Num = np.ones(numWords)  <span class="comment"># 同上</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span>  <span class="comment"># 分母初始化为2</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:  <span class="comment"># 如果是垃圾邮件</span></span><br><span class="line">            p1Num += trainMatrix[i]  <span class="comment"># 把属于同一类的文本向量相加，实质是统计某个词条在该类文本中出现频率</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])  <span class="comment"># 把垃圾邮件向量的所有元素加起来，表示垃圾邮件中的所有词汇</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    p1 = np.log(p1Num / p1Denom)  <span class="comment"># 统计词典中所有词条在垃圾邮件中出现的概率</span></span><br><span class="line">    p0 = np.log(p0Num / p0Denom)  <span class="comment"># 统计词典中所有词条在正常文邮件中出现的概率</span></span><br><span class="line">    <span class="keyword">return</span> p0,p1,pSpam</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>计算spam、ham各占总邮件数的概率p_spam、p_ham</li>
<li>计算待测试集的特征集向量test_vec</li>
<li>将<code>test_vec*spam_vec*p_spam</code> 与<code>test_vec*ham_vec*p_ham</code>相比</li>
<li>哪方概率大则该封测试邮件属于哪一类（为了避免正常邮件分为垃圾邮件，当概率相等时，判定为正常）</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_classify</span><span class="params">(test,features,ham_vec,spam_vec,p_spam)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(test, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        test_lines = f1.readlines()</span><br><span class="line">    line_i=<span class="number">1</span> <span class="comment">#用于判断计算到了第几封test，test集中，前50为正常，后50为垃圾</span></span><br><span class="line">    TP=<span class="number">0</span> <span class="comment">#正确肯定</span></span><br><span class="line">    TN=<span class="number">0</span> <span class="comment">#正确否定</span></span><br><span class="line">    FP=<span class="number">0</span> <span class="comment">#错误肯定</span></span><br><span class="line">    FN=<span class="number">0</span> <span class="comment">#错误否定</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> test_lines: <span class="comment">#计算test集邮件的词向量，并且判断正常\垃圾</span></span><br><span class="line">        test_vec = np.zeros(len(features))</span><br><span class="line">        line = i.split(<span class="string">' '</span>)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(features)):</span><br><span class="line">            <span class="keyword">for</span> line_feature <span class="keyword">in</span> line :</span><br><span class="line">                <span class="keyword">if</span> features[j] == line_feature:</span><br><span class="line">                    test_vec[j]=test_vec[j]+<span class="number">1</span></span><br><span class="line">        pnorm=sum(test_vec*ham_vec*p_spam)</span><br><span class="line">        pabu=sum(test_vec*spam_vec*p_spam)</span><br><span class="line">        <span class="comment"># if (line_i == 47):</span></span><br><span class="line">        <span class="comment">#     print(test_vec)</span></span><br><span class="line">        <span class="comment"># if(line_i==51): #如果开始判断垃圾邮件就分下行</span></span><br><span class="line">            <span class="comment"># print("————————————————————————————————————————")</span></span><br><span class="line">        <span class="keyword">if</span> pnorm&gt;=pabu : <span class="comment">#正常概率大（ps:当概率相等时，判定为正常)</span></span><br><span class="line">            <span class="keyword">if</span> line_i&gt;<span class="number">50</span> :</span><br><span class="line">                <span class="comment"># print("第%d封邮件是正常邮件" % line_i, end='')</span></span><br><span class="line">                <span class="comment"># print("————判断错误！————"+"其实是垃圾邮件哒!",end='')</span></span><br><span class="line">                FP=FP+<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># print("第%d封邮件是正常邮件" % line_i, end='')</span></span><br><span class="line">                TP =TP+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> line_i &lt; <span class="number">51</span>:</span><br><span class="line">                <span class="comment"># print("————判断错误！————" + "其实是正常邮件哒",end='')</span></span><br><span class="line">                TN=TN+<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># print("第%d封邮件是错误邮件" % line_i, end='')</span></span><br><span class="line">                FN=FN+<span class="number">1</span></span><br><span class="line">        <span class="comment"># print("   正常概率VS错误概率：",end='')</span></span><br><span class="line">        <span class="comment"># print(pnorm,pabu)</span></span><br><span class="line">        line_i+=<span class="number">1</span></span><br><span class="line">    print(<span class="string">"featuears项数："</span>,len(features))</span><br><span class="line">    print(<span class="string">"正确肯定：预测为真，实际为真"</span>,TP)</span><br><span class="line">    print(<span class="string">"正确否定：预测为假，实际为真"</span>,TN)</span><br><span class="line">    print(<span class="string">"错误肯定：预测为真，实际为假"</span>,FP)</span><br><span class="line">    print(<span class="string">"错误否定：预测为假，实际为假"</span>,FN)</span><br><span class="line">    P=TP/(TP/FP) <span class="comment">#查准率</span></span><br><span class="line">    R=TP/(TP+FN) <span class="comment">#查全率</span></span><br><span class="line">    ACC=(TP+FN)/(TP+TN+FP+FN)</span><br><span class="line">    F = <span class="number">2</span>*TP / (<span class="number">2</span>*TP+FP+FN)</span><br><span class="line">    print(<span class="string">"查准率P=TP/（TP+FP）:"</span>,P)</span><br><span class="line">    print(<span class="string">"查全率R=TP/（TP+FN）:"</span>,R)</span><br><span class="line">    print(<span class="string">"准确率ACC=(TP+FN) / (TP+TN+FP+FN):"</span>,ACC)</span><br><span class="line">    print(<span class="string">"调和均值F= 2TP / (2TP+FP+FN):"</span>,F)</span><br></pre></td></tr></table></figure>

<p>main函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    number_of_item=[<span class="number">10</span>,<span class="number">25</span>,<span class="number">50</span>,<span class="number">75</span>,<span class="number">100</span>,<span class="number">150</span>,<span class="number">200</span>,<span class="number">300</span>,<span class="number">500</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> number_of_item:</span><br><span class="line">        print(<span class="string">"————————————————————————————————————————"</span>)</span><br><span class="line">        print(<span class="string">"特征项数为："</span>,i)</span><br><span class="line">        extract_tags_f(<span class="string">"test.utf8"</span>, <span class="string">"test_word.txt"</span>,i)  </span><br><span class="line">        <span class="comment">#对测试集分词</span></span><br><span class="line">        features = extract_tags_f(<span class="string">"ham_100.utf8"</span>, <span class="string">"ham_word.txt"</span>,i) </span><br><span class="line">        <span class="comment">#对正常邮件分词，并提取词向量</span></span><br><span class="line">        features += extract_tags_f(<span class="string">"spam_100.utf8"</span>, <span class="string">"spam_word.txt"</span>,i) </span><br><span class="line">        <span class="comment">#垃圾邮件分词，并提取词向量</span></span><br><span class="line">        duplicated = set()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(features)):</span><br><span class="line">            <span class="keyword">if</span> features[i] <span class="keyword">in</span> features[i+<span class="number">1</span>:]:</span><br><span class="line">                duplicated.add(features[i]) </span><br><span class="line">                <span class="comment">#寻找垃圾邮件和正常邮件重复的特征词</span></span><br><span class="line">        <span class="comment"># print(duplicated)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(features) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> features[i] <span class="keyword">in</span> duplicated:</span><br><span class="line">                features.remove(features[i]) </span><br><span class="line">                <span class="comment">#去除垃圾邮件和正常邮件重复的特征词，但是感觉效果并不好</span></span><br><span class="line">        <span class="comment"># print(features)</span></span><br><span class="line">        print(<span class="string">"去除重复词后特征项数为："</span>,len(features))</span><br><span class="line">        list_sum,cate=calc_vec(<span class="string">"ham_word.txt"</span>,<span class="string">"spam_word.txt"</span>,features) <span class="comment">#计算总体词向量</span></span><br><span class="line">        ham_vec, spam_vec,p_spam=traing_bayes(list_sum,cate) <span class="comment">#计算条件概率，以及先验概率</span></span><br><span class="line">        test_classify(<span class="string">"test_word.txt"</span>,features,ham_vec, spam_vec,p_spam) <span class="comment">#分类</span></span><br></pre></td></tr></table></figure>

<p>运行程序：</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416165856310.png" alt="image-20200416165856310"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>朴素贝叶斯分类算法评价：</p>
<p><strong>查准率（Precision）、查全率（召回率）（Recall）</strong>、<strong>准确率(Accuracy)</strong></p>
<p>我们将算法预测的结果分成四种情况：</p>
<ol>
<li><p><strong>正确肯定</strong>（True Positive,TP）：预测为真，实际为真</p>
</li>
<li><p><strong>正确否定</strong>（True Negative,TN）：预测为假，实际为真</p>
</li>
<li><p><strong>错误肯定</strong>（False Positive,FP）：预测为真，实际为假</p>
</li>
<li><p><strong>错误否定</strong>（False Negative,FN）：预测为假，实际为假</p>
</li>
</ol>
<p>则：</p>
<p><strong>查准率P=TP/（TP+FP）</strong></p>
<p><strong>查全率R=TP/（TP+FN）</strong></p>
<p><strong>准确率ACC=(TP+FN) / (TP+TN+FP+FN)</strong></p>
<p>以下为去除重读词</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">去除重复词后特征项数为： 20</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.86</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.647887323943662</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 48</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.9</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6575342465753424</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 96</span><br><span class="line">查准率P&#x3D;TP&#x2F;（TP+FP）: 10.0</span><br><span class="line">查全率R&#x3D;TP&#x2F;（TP+FN）: 0.550561797752809</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.89</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6621621621621622</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 142</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.92</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6666666666666666</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 192</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.92</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6666666666666666</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 290</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.92</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6666666666666666</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 386</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.92</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6666666666666666</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 578</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.93</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6666666666666666</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">去除重复词后特征项数为： 924</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.92</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6666666666666666</span><br></pre></td></tr></table></figure>

<p>可以看到调和均值F趋近于极限了，再加特征项数也无用</p>
<p>如果不去除重读词，反而会好些，最好结果如下，特征项数为150</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416171617463.png" alt="image-20200416171617463"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">特征项数为： 75</span><br><span class="line">去除重复词后特征项数为： 150</span><br><span class="line">featuears项数： 150</span><br><span class="line">正确肯定：预测为真，实际为真 49</span><br><span class="line">正确否定：预测为假，实际为真 1</span><br><span class="line">错误肯定：预测为真，实际为假 5</span><br><span class="line">错误否定：预测为假，实际为假 45</span><br><span class="line">查准率P&#x3D;TP&#x2F;（TP+FP）: 5.0</span><br><span class="line">查全率R&#x3D;TP&#x2F;（TP+FN）: 0.5212765957446809</span><br><span class="line">准确率ACC&#x3D;(TP+FN) &#x2F; (TP+TN+FP+FN): 0.94</span><br><span class="line">调和均值F&#x3D; 2TP &#x2F; (2TP+FP+FN): 0.6621621621621622</span><br></pre></td></tr></table></figure>

<p>错误的基本是如下6封，不过如果去除重复词那就不会有将正常邮件判断成错误邮件的情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">第29封邮件是错误邮件————判断错误！————其实是正常邮件哒   </span><br><span class="line">正常概率VS错误概率：-44.397541630442696 -44.09868095860785</span><br><span class="line">————————————————————————————————————————</span><br><span class="line">第64封邮件是正常邮件————判断错误！————其实是垃圾邮件哒!   </span><br><span class="line">正常概率VS错误概率：-13.513755442998768 -13.7612297041614</span><br><span class="line">第70封邮件是正常邮件————判断错误！————其实是垃圾邮件哒!   </span><br><span class="line">正常概率VS错误概率：-13.513755442998768 -13.7612297041614</span><br><span class="line">第73封邮件是正常邮件————判断错误！————其实是垃圾邮件哒!   </span><br><span class="line">正常概率VS错误概率：-6.9073094336244685 -6.939395461206766</span><br><span class="line">第92封邮件是正常邮件————判断错误！————其实是垃圾邮件哒!   </span><br><span class="line">正常概率VS错误概率：-54.79272850055003 -55.26584135501914</span><br><span class="line">第94封邮件是正常邮件————判断错误！————其实是垃圾邮件哒!   </span><br><span class="line">正常概率VS错误概率：-54.79272850055003 -55.26584135501914</span><br></pre></td></tr></table></figure>

<p>其中70\73都比较特殊，其邮件字数都很少，难以分类成功</p>
<p><img src="/2020/04/02/2020-04-02-inforamtion-security-exp-2/image-20200416172425856.png" alt="image-20200416172425856"></p>
<p>理论上来说，对于贝叶斯而言：</p>
<p><strong>主要优点</strong>有：</p>
<p>1）朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。 </p>
<p>2）对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。 </p>
<p>3）对缺失数据不太敏感，算法也比较简单，常用于文本分类。</p>
<p><strong>主要缺点</strong>有：</p>
<p>1） 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下，假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。 </p>
<p>2）需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。 </p>
<p>3）由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。 </p>
<p>4）对输入数据的表达形式很敏感。</p>
<p>而综合实验：</p>
<p>对于邮件分类任务而言，贝叶斯确实蛮稳的，在如此少量的数据集中，有稳定的、有效的分类效率，对小规模的数据表现很好。</p>
<p>而其在这个数据集中的准确率ACC最高可达94%，基本无法再提高。</p>
<p>我个人认为原因主要有如下几点：</p>
<ul>
<li>训练集不够多，不够贴近实际</li>
<li>测试集存在特殊情况，如邮件极短、邮件较独特等</li>
<li>用于训练的特征项数，我上面的实验已经得出了结论</li>
</ul>
<h1 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h1><p>这次的实验还是收获很多的：</p>
<ul>
<li>学会了使用jieba库进行分词，提取特征词</li>
<li>学会了使用gensim进行word2vec词向量话</li>
<li>最大的收获是，自己实操，对贝叶斯算法进行了一次深入的尝试，很愉悦~</li>
<li>对文本分类的流程、机器学习的理解更深了</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/exp/" rel="tag"><i class="fa fa-tag"></i> exp</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/25/2020-03-25-information-security-exp-1/" rel="next" title="2020-03-25-information-security-exp-1">
                <i class="fa fa-chevron-left"></i> 2020-03-25-information-security-exp-1
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/04/30/2020-04-30-algorithm-homework-2/" rel="prev" title="2020-04-30-algorithm-homework-2">
                2020-04-30-algorithm-homework-2 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div id="gitalk-container">
	</div>	

  




        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="ColaLinN" />
            
              <p class="site-author-name" itemprop="name">ColaLinN</p>
              <p class="site-description motion-element" itemprop="description">集中一点 登峰造极</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/ColaLinN" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:shenqiaaa@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#实验环境"><span class="nav-text">实验环境</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实验1-1-分词与词向量化"><span class="nav-text">实验1.1-分词与词向量化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#背景介绍"><span class="nav-text">背景介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-分词"><span class="nav-text">1.分词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-词向量化"><span class="nav-text">2.词向量化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验"><span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-分词-1"><span class="nav-text">1.分词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-词向量化"><span class="nav-text">1.词向量化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实验1-2-自选词典数据语料库"><span class="nav-text">实验1.2-自选词典数据语料库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-选择词典数据语料库"><span class="nav-text">1.选择词典数据语料库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-分词"><span class="nav-text">2.分词</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-词向量化"><span class="nav-text">3.词向量化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实验2-垃圾邮件的分类"><span class="nav-text">实验2-垃圾邮件的分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#原理"><span class="nav-text">原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-文本分类"><span class="nav-text">1.文本分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-朴素贝叶斯分类算法"><span class="nav-text">2.朴素贝叶斯分类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-概率计算"><span class="nav-text">(1).概率计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-“朴素”——引入条件独立性假设"><span class="nav-text">(2).“朴素”——引入条件独立性假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-假设每个样本至少出现一次"><span class="nav-text">(3).假设每个样本至少出现一次</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-将全部乘法运算改为log运算"><span class="nav-text">(4).将全部乘法运算改为log运算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-实际的编码流程"><span class="nav-text">3.实际的编码流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结-1"><span class="nav-text">总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ColaLinN</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">站点总字数&#58;</span>
    
    <span title="站点总字数">102.5k</span>
  
</div>







<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
    有<span id="busuanzi_value_site_uv"></span>人访问
</span>
</div>



<!--添加网站的运行时间-->
<span id="sitetime"></span>
<script language=javascript>
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数 */
		var t1 = Date.UTC(2020,01,15,19,00,00); //北京时间2018-2-13 00:00:00
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" 已安全运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
	}/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
	siteTime();
</script>


        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '7dd0e4dd1112b4f6ac75',
          clientSecret: '5f52c7a06abafb34741cd6468191b708c1f166b9',
          repo: 'ColaLinN.github.io',
          owner: 'ColaLinN',
          admin: ['ColaLinN'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>


  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

    <!-- 代码块复制功能 -->
  <script type="text/javascript" src="/js/src/clipboard.min.js"></script>  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>
